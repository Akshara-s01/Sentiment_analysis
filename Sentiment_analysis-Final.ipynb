{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd56404f-030e-491f-a42e-60d289df6330",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c071649-fa72-4f65-ace9-bf75b8097d15",
   "metadata": {},
   "source": [
    "This notebook presents a comprehensive approach to binary sentiment classification using the IMDB movie reviews dataset, which contains 50,000 labeled reviews (25,000 for training and 25,000 for testing). The goal is to classify each review as either positive or negative.\n",
    "\n",
    "We explore traditional NLP techniques and deep learning models by:\n",
    "\n",
    "* Preprocessing text data using NLTK or spaCy,\n",
    "\n",
    "* Generating embeddings using TF-IDF and Word2Vec,\n",
    "\n",
    "* Training and evaluating models including Logistic Regression, LSTM, and BERT.\n",
    "\n",
    "Through this project, we aim to understand how different modeling techniques perform on real-world sentiment data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac13dd-bf89-4ec6-ace5-8be146f35cdd",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4709f4cf-9737-4444-b0ec-3165d75cb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19de8dd6-e401-4ee3-95ec-8a920a9ed39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3d334f-f9f7-4199-a5d4-33627deef706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "--------------------------------------------------------------------------------\n",
      "                                                  review sentiment\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n"
     ]
    }
   ],
   "source": [
    "# View first and last few records\n",
    "print(\"Dataset Overview:\")\n",
    "print(df.head())\n",
    "print(\"-\"*80)\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0f73de-3e21-49a3-a346-4b3e2352697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "                                                   review sentiment\n",
      "count                                               50000     50000\n",
      "unique                                              49582         2\n",
      "top     Loved today's show!!! It was a variety and not...  positive\n",
      "freq                                                    5     25000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View structure and stats\n",
    "print(df.info())\n",
    "print(\"-\"*80)\n",
    "print(df.describe())\n",
    "print(\"-\"*80)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a90a051-2e1c-47b1-bde1-fa86e1b8b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Distribution:\n",
      "\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check class balance\n",
    "print(\"Sentiment Distribution:\\n\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"-\"*80)\n",
    "print(df.isnull().sum())\n",
    "print(\"-\"*80)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea333c04-3990-40c7-bea0-6bcfae6d9ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ayumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7fc237-70f8-48cb-a3f2-4de18ac97c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67898b58-f896-4a52-9643-b8d89565a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5331d28-50f5-4ef5-92b6-50db252c2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                        clean_review  \n",
      "0  one reviewer mentioned watching oz episode you...  \n",
      "1  wonderful little production filming technique ...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  basically there family little boy jake think t...  \n",
      "4  petter matteis love time money visually stunni...  \n"
     ]
    }
   ],
   "source": [
    "# Apply to a small sample first to test\n",
    "sample_df = df.head(5).copy()\n",
    "sample_df['clean_review'] = sample_df['review'].apply(preprocess_text)\n",
    "print(sample_df[['review', 'clean_review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "582a78b9-a5a6-440c-be6e-956e575924b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:54<00:00, 912.40it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['clean_review'] = df['review'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d73c81-5d0e-445f-8e81-aa9b37337e41",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization (classic and fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "525a315f-cb2b-41ca-9328-07e512fc6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['clean_review'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4e551-937d-43c6-b61c-63c612362724",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings (contextual word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8602ea24-533a-489d-a3b2-9ab9324319da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14054464  0.26114455  0.2861747  -0.01132216 -1.9468822  -0.01961933\n",
      " -0.12557174 -0.5686028  -0.57852674 -2.7938473   0.01985246 -1.2567481\n",
      "  2.3373716  -0.18140516 -0.45398542 -1.0842849   3.025554    0.69989866\n",
      " -1.608329   -1.2478198   1.2674246  -1.2045821   1.9984994  -0.03028275\n",
      "  1.314082   -1.9821984  -0.19048803 -0.26103324 -0.37805805  1.351212\n",
      "  0.39358902 -0.7836576   1.6210145  -1.312569   -0.6867903   0.22486292\n",
      " -0.74402297 -0.42990008  0.31368807  2.4010155  -3.5769436   0.639443\n",
      "  0.40140754  1.286355   -0.07115585  0.9736012  -0.24862228 -1.8691294\n",
      "  1.4374658   0.22365648 -1.8970941  -0.24963419 -1.5368038   1.7550664\n",
      "  2.9744265   0.6803544   0.4785899   0.03902685 -0.7566226   0.3237298\n",
      "  0.22365047 -0.2927757   1.4900663  -0.49961895  0.53170764  0.6116781\n",
      "  0.34949964  1.5971588  -1.4182926   0.40802798 -0.21277958 -0.35354707\n",
      " -0.13932127  0.11379347 -0.57939005  0.20816168 -0.1943101   1.4107716\n",
      " -0.11497167 -0.9540536   0.14611368 -0.31462672 -1.9710107   2.2912695\n",
      "  0.10145731  0.29141542  0.34167266 -0.5135703  -0.18583527 -0.6857913\n",
      "  0.3031396  -1.0252566  -0.16593306 -0.00913587 -0.1061511   0.9462463\n",
      " -1.2604026  -0.15588784  1.1715125  -0.68657947]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare tokenized sentences (list of tokens per review)\n",
    "tokenized_reviews = [review.split() for review in df['clean_review']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "print(w2v_model.wv['movie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9cd76-5264-47ca-8b03-70410628473c",
   "metadata": {},
   "source": [
    "# Create document vectors for each review by averaging word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eef11c10-e2de-4aea-839b-ae2f5e7b0980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec embedding matrix shape: (50000, 100)\n"
     ]
    }
   ],
   "source": [
    "def get_w2v_vector(tokens, model, vector_size=100):\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vec += model.wv[token]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# Apply to all reviews\n",
    "X_w2v = np.array([get_w2v_vector(review.split(), w2v_model) for review in df['clean_review']])\n",
    "\n",
    "print(f\"Word2Vec embedding matrix shape: {X_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa45555-6f3f-44a7-9304-d775ba691625",
   "metadata": {},
   "source": [
    "## Logistic Regression Training & Evaluation (Using TF-IDF Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e49536-cce5-4227-bea8-f04b54a361d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8845\n",
      "--------------------------------------------------------------------------------\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.87      0.88      4961\n",
      "    positive       0.88      0.90      0.89      5039\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "df['label'] = df['sentiment'].map({'positive':1, 'negative':0})\n",
    "\n",
    "# Train-test split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Predictions\n",
    "y_pred1 = lr_model.predict(X_test1)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test1, y_pred1):.4f}\")\n",
    "print(\"-\"*80)\n",
    "print(\"classification report:\\n\", classification_report(y_test1, y_pred1, target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb72120-62bd-4b08-b9a1-92289377cd2c",
   "metadata": {},
   "source": [
    "## LSTM Model with PyTorch using Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72820e36-ada6-4737-bae1-c387c11a6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e23dd08-aa9c-481b-a995-05af92d06250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from Word2Vec model\n",
    "vocab = {word: idx+1 for idx, word in enumerate(w2v_model.wv.index_to_key)}\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "# Convert reviews to list of word indices\n",
    "def text_to_indices(text):\n",
    "    return [vocab.get(word, 0) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ba5e1f6-a9a3-4f5f-adef-dec86b3935b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for sentiment data\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = [torch.tensor(text_to_indices(t)) for t in texts]\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Function to pad sequences within a batch\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return texts_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dff326de-3b58-4e69-a4df-fb5076a47980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train_texts, X_test_texts, y_train_labels, y_test_labels = train_test_split(df['clean_review'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "737e8d0e-071f-4d7d-b6ce-4a2db9726c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "train_dataset = SentimentDataset(X_train_texts, y_train_labels)\n",
    "test_dataset = SentimentDataset(X_test_texts, y_test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73018e07-c489-4879-858c-413fe1867b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM-based classification model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, w2v_model, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        # Prepare embedding matrix\n",
    "        pretrained_vectors = w2v_model.wv.vectors\n",
    "        vocab_size = pretrained_vectors.shape[0] + 1  # +1 for padding\n",
    "\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "        embedding_matrix[1:] = pretrained_vectors  # shift by 1 to reserve idx 0 for padding\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = True  # Set False to freeze embeddings\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5894b838-7ffb-4c01-9e8d-68d1d9f62919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # binary classification\n",
    "\n",
    "# Instantiate the model and move it to the device\n",
    "model = LSTMClassifier(w2v_model, embedding_dim, hidden_dim, output_dim)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32334114-be4d-48f0-844c-61fcb10d564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8c0a469-ad1f-4791-bc4b-80ae3a58456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts).squeeze(1)\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f0e9a51-60b4-4767-ac1a-f3986fa209cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6925, Validation Accuracy = 0.5073\n",
      "Epoch 2: Train Loss = 0.6714, Validation Accuracy = 0.5067\n",
      "Epoch 3: Train Loss = 0.6423, Validation Accuracy = 0.7152\n",
      "Epoch 4: Train Loss = 0.6470, Validation Accuracy = 0.5015\n",
      "Epoch 5: Train Loss = 0.6572, Validation Accuracy = 0.7608\n",
      "Epoch 6: Train Loss = 0.5330, Validation Accuracy = 0.7935\n",
      "Epoch 7: Train Loss = 0.5282, Validation Accuracy = 0.7960\n",
      "Epoch 8: Train Loss = 0.5061, Validation Accuracy = 0.4995\n",
      "Epoch 9: Train Loss = 0.6898, Validation Accuracy = 0.5071\n",
      "Epoch 10: Train Loss = 0.6418, Validation Accuracy = 0.5052\n",
      "Epoch 11: Train Loss = 0.5676, Validation Accuracy = 0.7982\n",
      "Epoch 12: Train Loss = 0.5311, Validation Accuracy = 0.8343\n",
      "Epoch 13: Train Loss = 0.3532, Validation Accuracy = 0.8650\n",
      "Epoch 14: Train Loss = 0.2733, Validation Accuracy = 0.8756\n",
      "Epoch 15: Train Loss = 0.2416, Validation Accuracy = 0.8772\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    loss = train(model, train_loader)\n",
    "    preds, labels = evaluate(model, test_loader)\n",
    "    acc = (preds == labels).mean()\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {loss:.4f}, Validation Accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71c3edf1-d66b-4042-80c9-1ab76320f8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8902    0.8583    0.8740      4961\n",
      "         1.0     0.8652    0.8958    0.8803      5039\n",
      "\n",
      "    accuracy                         0.8772     10000\n",
      "   macro avg     0.8777    0.8771    0.8771     10000\n",
      "weighted avg     0.8776    0.8772    0.8771     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4258  703]\n",
      " [ 525 4514]]\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc993c56-154b-4912-af02-10b56b7b05a3",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da23cf8e-dec0-417c-bb64-313a926e270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db3ed0ad-fef8-4c1e-8116-ab1568157e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode the input texts using BERT tokenizer\n",
    "def encode_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf3fe0e9-0126-48bc-800d-7c747ecb0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset class to hold BERT inputs and labels\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d15cd5d-1a93-481a-9d72-8389c1336067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_review'], df['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65ea17b5-217d-4ddd-ba50-292d4b5d4985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the training and test data\n",
    "train_encodings = encode_texts(X_train)\n",
    "test_encodings = encode_texts(X_test)\n",
    "\n",
    "# Create PyTorch dataset objects\n",
    "train_dataset = BERTDataset(train_encodings, y_train)\n",
    "test_dataset = BERTDataset(test_encodings, y_test)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8063f36f-dbb4-4d32-978c-200c8edd74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a69d01f4-4d9f-4fea-9331-5a7f3d199dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "038f11c2-111a-429f-8e61-a45071ce69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function to compute accuracy\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "007bf805-89ed-4a2b-a6ba-51860331a058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2500/2500 [19:43<00:00,  2.11it/s]\n",
      "Evaluating: 100%|██████████| 625/625 [01:27<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.3188, Val Accuracy = 0.8917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2500/2500 [19:02<00:00,  2.19it/s]\n",
      "Evaluating: 100%|██████████| 625/625 [01:27<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.1813, Val Accuracy = 0.8974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2500/2500 [19:01<00:00,  2.19it/s]\n",
      "Evaluating: 100%|██████████| 625/625 [01:28<00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.0852, Val Accuracy = 0.8958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model and evaluate after each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader)\n",
    "    val_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {train_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e39d2b01-caf0-4d89-b73d-d1776fefd534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions and true labels for evaluation metrics\n",
    "def get_predictions(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "411a6679-a77a-446c-8cb3-b23c6bf8f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4483  514]\n",
      " [ 528 4475]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8946    0.8971    0.8959      4997\n",
      "           1     0.8970    0.8945    0.8957      5003\n",
      "\n",
      "    accuracy                         0.8958     10000\n",
      "   macro avg     0.8958    0.8958    0.8958     10000\n",
      "weighted avg     0.8958    0.8958    0.8958     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report and confusion matrix\n",
    "preds, labels = get_predictions(model, test_loader)\n",
    "print(confusion_matrix(labels, preds))\n",
    "print(classification_report(labels, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acd5ab65-2f8a-48c0-b9d2-84f64de5f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment for a single input text\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4aeaeae-9762-402f-ae1a-383fe50b0112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "print(predict_sentiment(\"This movie was an absolute masterpiece.\"))\n",
    "print(predict_sentiment(\"This movie was an absolute disaster.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f0c17-6a0e-44c6-97dd-80abf41c579e",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1db9f-0d72-4f1e-8057-ed099e86febc",
   "metadata": {},
   "source": [
    "In this project, we explored sentiment analysis on the IMDB 50K movie reviews dataset using a combination of classical machine learning and deep learning approaches. Starting with robust text preprocessing using NLTK/spaCy, we generated meaningful text embeddings via TF-IDF and Word2Vec, enabling effective sentiment classification.\n",
    "\n",
    "Our experiments revealed that:\n",
    "\n",
    "* Logistic Regression with TF-IDF yielded a strong baseline with 88.45% accuracy, showcasing the enduring power of linear models when paired with good feature engineering.\n",
    "\n",
    "* LSTM models demonstrated the potential of sequence-aware architectures by capturing word dependencies, achieving 87.72% accuracy. This underlined the strength of deep learning models in handling temporal text data.\n",
    "\n",
    "* BERT, a state-of-the-art transformer-based model, outperformed both by achieving 89.58% accuracy, thanks to its contextualized word embeddings and deep language understanding capabilities.\n",
    "\n",
    "The consistent performance across all three models reinforces the idea that while traditional models are fast and effective, modern deep learning and transformer-based approaches provide superior accuracy and generalization, especially for nuanced NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37158ebd-f16f-4bc6-b85b-6f9d582776e4",
   "metadata": {},
   "source": [
    " -By Akshara S, Data Science Intern at inGrade"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sentiment NLP",
   "language": "python",
   "name": "sentiment-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
